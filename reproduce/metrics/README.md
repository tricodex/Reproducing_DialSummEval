## Metrics

This directory contains the metrics that were applied in the orignal paper, some of which are organized into individual directories containing a `my_metric.py` file that executes the metric to aquire the metric score on the summary. The remaining metrics can be executed in the `other_metrics_and_human.py` file. This file contains the remaining metrics:

- [SummaQA](https://github.com/ThomasScialom/summa-qa)
- [nlg-eval](https://github.com/Maluuba/nlg-eval#readme) (
    BLEU,
    METEOR,
    ROUGE, 
    Embedding Average,
    Vector Extrema,
    Greedy Matching)
 - [QuestEval](https://github.com/ThomasScialom/QuestEval)
 - [ROUGE](https://pypi.org/project/rouge/)
 - [BLANC](https://pypi.org/project/blanc/)
 - [BERTScore](https://pypi.org/project/bert-score/)
 - [PPL](https://huggingface.co/docs/transformers/perplexity)

NOTE: All metrics scores are stored in `.\DialSummEvalVU\reproduce\analysis\models_eval_new` A-N. <br />
NOTE: Carefully follow the instruction for each metric, they are all provided with a link or a README.md. <br />
NOTE: Depending on the operating system (e.g. Linux/Windows), several paths within the code need to be adjusted, beware of \ and /. 

### Categorization

An overview of each metric in their respective category with references to the original work:

#### Metrics based on n-gram overlap:

- [ROUGE (Lin, 2004)](https://aclanthology.org/W04-1013) measures the unigram-overlap (ROUGE-1), bigram-overlap (ROUGE-2) and longest common sequence (ROUGE-L) between two texts.
- [BLEU (Papineni et al., 2002)](https://doi.org/10.3115/1073083.1073135) calculates n-gram overlap between texts using precision scores and includes a brevity penalty.
- [METEOR (Banerjee and Lavie, 2005)](https://aclanthology.org/W05-0909) computes an alignment by mapping unigrams in two texts, based on surface forms, stemmed forms, and meanings.
- [CHRF (PopoviÄ‡, 2015)](https://doi.org/10.18653/v1/W15-3049) computes character-based n-gram overlap between two texts.

#### Metrics based on pre-trained language models:

- [BERTScore (Zhang et al., 2020)](https://openreview.net/forum?id=SkeHuCVFDr) measures the soft-overlap between two texts at the token level using contextual embeddings from BERT.
- [MoverScore (Zhao et al., 2019)](https://doi.org/10.18653/v1/D19-1053) applies the semantic distance between two texts at the n-gram level using n-gram embeddings pooled from BERT.
- [BARTScore (Yuan et al., 2021)](https://arxiv.org/abs/2106.11520) evaluates the quality of a generated text by assuming that the quality is better when the conditional language model has a higher probability of generating it or the reference, or is more likely to generate the reference from it.
- [BLANC (Vasilyev et al., 2020)](https://doi.org/10.18653/v1/2020.eval4nlp-1.2) measures the performance boost of masked language modeling for BERT utilizing the summary in two different ways, and is referenceless.
- [PPL](https://huggingface.co/docs/transformers/perplexity) is often used to evaluate the quality of a language model or the fluency of an utterance.

#### Metrics based on word embeddings:

- [SMS (Sentence Mover Similarity, Clark et al., 2019)](https://doi.org/10.18653/v1/P19-1264) measures the distance between two texts represented as a bag of sentence embeddings.
- [Embedding average (Landauer and Dumais, 1997)](https://www.stat.cmu.edu/~cshalizi/350/2008/readings/Landauer-Dumais.pdf) computes the cosine similarity between the embeddings of two texts, where a sentence-level embedding is represented by averaging the embeddings of the words composing the sentence.
- [Vector extrema (Forgues et al., 2014)](http://www.cs.cmu.edu/~apparikh/nips2014ml-nlp/camera-ready/forgues_etal_mlnlp2014.pdf) computes a sentence-level embedding by taking the most extreme value of the embeddings of the words composing the sentence for each dimension of the embedding.
- [Greedy matching (Rus and Lintean, 2012)](https://aclanthology.org/W12-2018) directly compares the embeddings of words in two sentences using a greedy matching algorithm to calculate similarity.

#### Metrics based on question-answering:

- [FEQA (Durmus et al., 2020)](https://doi.org/10.18653/v1/2020.acl-main.454) uses a BERT-based question-answering model to answer questions using the source document, where questions are generated by a fine-tuned BART model using generated summaries with masked named entities as inputs.
- [SummaQA (Scialom et al., 2019)](https://doi.org/10.18653/v1/D19-1320) generates questions from source documents and uses summaries to answer them. The F1 overlap score and QA-model confidence are reported.
- [QuestEval (Scialom et al., 2021)](https://doi.org/10.18653/v1/2021.emnlp-main.529) takes into account the scores obtained from FEQA and SummaQA. It uses the reference-less mode for comparison purposes.

#### Metrics based on entailment classification:
- [FactCC (Kryscinski et al., 2020)](https://doi.org/10.18653/v1/2020.emnlp-main.750) determines the consistency of the summary by feeding each sentence of the summary into a classifier along with the document. The proportion of consistent sentences is used to indicate the consistency of the summary.
- DAE [(Goyal and Durrett, 2021](https://doi.org/10.18653/v1/2021.naacl-main.114); [Goyal and Durrett, 2020)](https://doi.org/10.18653/v1/2020.findings-emnlp.322) uses a similar approach as FactCC, and when a sentence cannot be parsed by the metric, it is defaulted as factually inconsistent.
